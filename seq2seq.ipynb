{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumedhGanpatye/Seq2Seq/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgSPiPhWBlzH",
        "outputId": "115164cc-6845-44cf-ef07-5c0e8babb20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYMuVvOmokh0",
        "outputId": "0115b9f7-c861-4186-eb13-a6c579656253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6.0\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8z760EYN2P3",
        "outputId": "70ea7ce4-abdf-4797-eefe-bb5e99745ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-YBsSFRvrj7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.data import BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VGZlnKOwnSo"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy_ger = spacy.load('de_core_news_sm')\n",
        "spacy_eng = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWQTeNLPxB9Y"
      },
      "outputs": [],
      "source": [
        "def tokenizer_ger(text):\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "def tokenizer_eng(text):\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JSVjCoFdngs"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    model.eval()\n",
        "    tokens = [token.text.lower() for token in german.tokenizer(sentence)]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:-1]\n",
        "\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        outputs.append(prediction)\n",
        "        targets.append([trg])\n",
        "\n",
        "    return bleu_score(outputs, targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5a8L6VqSvyQ"
      },
      "outputs": [],
      "source": [
        "# def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "#     print(\"=> Saving checkpoint\")\n",
        "#     torch.save(state, filename)\n",
        "\n",
        "# def load_checkpoint(checkpoint, model, optimizer):\n",
        "#     print(\"=> Loading checkpoint\")\n",
        "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group[\"lr\"] = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD_8u1yWxLQ9"
      },
      "outputs": [],
      "source": [
        "# german = torchtext.data.Field(tokenize=tokenizer_ger,lower=True,init_token='<sos>',eos_token='<eos>')\n",
        "# english = torchtext.data.Field(tokenize=tokenizer_eng,lower=True,init_token='<sos>',eos_token='<eos>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7SKZUY3uV3V"
      },
      "outputs": [],
      "source": [
        "# train_data,validation_data,test_data = Multi30k.splits(exts=('de','en'),fields=(german,english))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "larnGBxx3ixY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7pFt5Mas2fp",
        "outputId": "b5e0fb95-5040-44f5-c237-74cc120d30a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "source": [
        "SRC_FIELD = torchtext.data.Field(tokenize='spacy', tokenizer_language='de_core_news_sm', init_token='<sos>', eos_token='<eos>')\n",
        "TRG_FIELD = torchtext.data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm', init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
        "    path='/content/drive/MyDrive/seq2seq_dataset', format='csv',\n",
        "    train='train.csv', validation='valid.csv', test='test.csv',\n",
        "    fields=[('src', SRC_FIELD), ('trg', TRG_FIELD)]\n",
        ")\n",
        "\n",
        "SRC_FIELD.build_vocab(train_data, min_freq=2)\n",
        "TRG_FIELD.build_vocab(train_data, min_freq=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk55r0L9yajJ"
      },
      "outputs": [],
      "source": [
        "german = SRC_FIELD\n",
        "english = TRG_FIELD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWaD2SMCxwIg"
      },
      "outputs": [],
      "source": [
        "german.build_vocab(train_data,max_size=10000,min_freq=2)\n",
        "english.build_vocab(train_data,max_size=10000,min_freq=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29sWMBV4x43_"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(english.vocab)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "fKLHItO4wpDQ",
        "outputId": "821040a7-f77b-4042-f4c9-f24ab115ea1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1 / 10]\n",
            "Translated example sentence: \n",
            " <sos> battling battling placard placard curly curly daring mountainside daring removed removed hotdogs hotdogs kilt dyed key troop placard fencing curly stacked sculpting flipped motorcycles sculpting motorcycles motorcycles lunch steaks steaks placard curly curly daring hotdogs hotdogs binoculars bulls arrange Capris basket basket curly curly daring everyone casino removed curly curly\n",
            "Epoch Loss: 647.3358535766602\n",
            "[Epoch 2 / 10]\n",
            "Translated example sentence: \n",
            " <sos> ' , ' . ' , ' . ' ] <eos>\n",
            "Epoch Loss: 521.9679753780365\n",
            "[Epoch 3 / 10]\n",
            "Translated example sentence: \n",
            " <sos> [ ' People ' , ' are ' , ' in ' , ' a ' , ' . ' , ' . ' ] <eos>\n",
            "Epoch Loss: 495.5521486401558\n",
            "[Epoch 4 / 10]\n",
            "Translated example sentence: \n",
            " <sos> [ ' Several ' , ' people ' , ' are ' , ' a ' , ' . ' ] <eos>\n",
            "Epoch Loss: 479.67281997203827\n",
            "[Epoch 5 / 10]\n",
            "Translated example sentence: \n",
            " <sos> [ ' People ' , ' are ' , ' in ' , ' a ' , ' . ' , ' . ' ] <eos>\n",
            "Epoch Loss: 465.3385348916054\n",
            "[Epoch 6 / 10]\n",
            "Translated example sentence: \n",
            " <sos> <unk> <unk> ' , ' . ' , ' . ' ] <eos>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1b5cd3b285e8>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "load_model = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 512\n",
        "decoder_embedding_size = 512\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.1\n",
        "dec_dropout = 0.1\n",
        "\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    model.eval()\n",
        "    tokens = german.tokenize(sentence)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    sentence_tensor = torch.tensor([german.vocab.stoi[token] for token in tokens]).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.tensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return ' '.join(translated_sentence)\n",
        "\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        ").to(device)\n",
        "\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
        "\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch + 1} / {num_epochs}]\")\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, german, english, device, max_length=50\n",
        "    )\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "        step += 1\n",
        "\n",
        "    print(f\"Epoch Loss: {epoch_loss}\")\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg6_saT30xpJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.data import BucketIterator\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Configuration settings\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "load_model = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "# Define the translation function\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    model.eval()\n",
        "    tokens = german.tokenize(sentence)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    sentence_tensor = torch.tensor([german.vocab.stoi[token] for token in tokens]).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.tensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return ' '.join(translated_sentence)  # Join tokens to form the complete string\n",
        "\n",
        "# Training configuration\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        ").to(device)\n",
        "\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "# Example sentence to translate\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch + 1} / {num_epochs}]\")\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, german, english, device, max_length=50\n",
        "    )\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "        step += 1\n",
        "\n",
        "    print(f\"Epoch Loss: {epoch_loss}\")\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EIzC7AsvNfR"
      },
      "outputs": [],
      "source": [
        "# num_epochs = 10\n",
        "# learning_rate = 0.001\n",
        "# batch_size = 64\n",
        "\n",
        "# load_model = False\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# input_size_encoder = len(german.vocab)\n",
        "# input_size_decoder = len(english.vocab)\n",
        "# output_size = len(english.vocab)\n",
        "# encoder_embedding_size = 300\n",
        "# decoder_embedding_size = 300\n",
        "# hidden_size = 1024\n",
        "# num_layers = 2\n",
        "# enc_dropout = 0.5\n",
        "# dec_dropout = 0.5\n",
        "\n",
        "# def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "\n",
        "#     model.eval()\n",
        "#     tokens = german.tokenize(sentence)\n",
        "#     tokens = [token.lower() for token in tokens]\n",
        "#     tokens.insert(0, german.init_token)\n",
        "#     tokens.append(german.eos_token)\n",
        "\n",
        "\n",
        "#     sentence_tensor = torch.tensor([german.vocab.stoi[token] for token in tokens]).unsqueeze(1).to(device)\n",
        "\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "#     outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "#     for _ in range(max_length):\n",
        "#         previous_word = torch.tensor([outputs[-1]]).to(device)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "#             best_guess = output.argmax(1).item()\n",
        "\n",
        "#         outputs.append(best_guess)\n",
        "\n",
        "\n",
        "#         if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "#             break\n",
        "\n",
        "#     translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "#     return translated_sentence[1:]\n",
        "\n",
        "\n",
        "# writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "# step = 0\n",
        "\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "#     (train_data, valid_data, test_data),\n",
        "#     batch_size=batch_size,\n",
        "#     sort_within_batch=True,\n",
        "#     sort_key=lambda x: len(x.src),\n",
        "#     device=device,\n",
        "# )\n",
        "\n",
        "# encoder_net = Encoder(\n",
        "#     input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        "# ).to(device)\n",
        "\n",
        "# decoder_net = Decoder(\n",
        "#     input_size_decoder,\n",
        "#     decoder_embedding_size,\n",
        "#     hidden_size,\n",
        "#     output_size,\n",
        "#     num_layers,\n",
        "#     dec_dropout,\n",
        "# ).to(device)\n",
        "\n",
        "# model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "# # criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "\n",
        "# sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
        "\n",
        "# losses = []\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "\n",
        "\n",
        "#     model.eval()\n",
        "#     translated_sentence = translate_sentence(\n",
        "#         model, sentence, german, english, device, max_length=50\n",
        "#     )\n",
        "#     print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "#     model.train()\n",
        "\n",
        "#     epoch_loss = 0\n",
        "#     for batch_idx, batch in enumerate(train_iterator):\n",
        "\n",
        "#         inp_data = batch.src.to(device)\n",
        "#         target = batch.trg.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(inp_data, target)\n",
        "#         output = output[1:].reshape(-1, output.shape[2])\n",
        "#         target = target[1:].reshape(-1)\n",
        "\n",
        "#         # optimizer.zero_grad()\n",
        "#         loss = criterion(output, target)\n",
        "#         epoch_loss += loss.item()\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "#         optimizer.step()\n",
        "\n",
        "#         writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "#         step += 1\n",
        "\n",
        "#     print(epoch_loss)\n",
        "#     losses.append(epoch_loss)\n",
        "\n",
        "# score = bleu(test_data[1:100], model, german, english, device)\n",
        "# print(f\"Bleu score {score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15VE1qIvzjrB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.data import BucketIterator\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Configuration settings\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "load_model = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size_encoder = len(german.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "# Define the translation function\n",
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    model.eval()\n",
        "    tokens = german.tokenize(sentence)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "\n",
        "    sentence_tensor = torch.tensor([german.vocab.stoi[token] for token in tokens]).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.tensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return ' '.join(translated_sentence)  # Join tokens to form the complete string\n",
        "\n",
        "# Training configuration\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        ").to(device)\n",
        "\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "# Example sentence to translate\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch + 1} / {num_epochs}]\")\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, german, english, device, max_length=50\n",
        "    )\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inp_data, target)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "        step += 1\n",
        "\n",
        "    print(f\"Epoch Loss: {epoch_loss}\")\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "score = bleu(test_data[1:100], model, german, english, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}